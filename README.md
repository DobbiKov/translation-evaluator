# Translation Evaluator

A python tool to evaluate machine translation. The tool evaluates:
- the Natural Language (NL) translation quality
- structure preserving quality

It supports common metrics (BLEU, chrF, TER) as well as recent neural metrics (BERTScore, COMET).

## Installation
1. Clone the repository:
```sh
git clone https://github.com/DobbiKov/translation-evaluator.git
cd translation-evaluator
```
2. Create the environment (with `conda`)
```sh
conda env create -f environment.yml 
```

## How to Use and Expand
1. Activate the environment (with `conda`)
    ```sh
    conda activate doc_eval_env
    ```

2. Populate `data/`:
    - Place your original scientific documents in `data/source/`.
    - Place the LLM-translated versions (with `_lang_code` suffix in filename, e.g., `my_doc_de.tex`) in `data/llm_translated/`.
    - (Optional but recommended for NL metrics) Place human-translated reference versions in data/human_reference/. (also with the `_lang_code`)

3. Configure `config.py`:
    - Adjust `LANG_PAIRS`, `DOC_FORMATS`.
    - Enable/disable specific metrics (`RUN_BLEU`, `RUN_AST_COMPARISON`, etc.) based on your needs.
    - Set `PANDOC_PATH` if **pandoc** isn't in your system's **PATH**.

4. Run `run_evaluation.py`:
```py
python3 run_evaluation.py
```

5. Review Reports:
- Check `reports/evaluation_results_*.json` for detailed results.
- Check `reports/evaluation_summary_*.csv` for a quick overview of NL and structural counts.
- For LaTeX visual diffs, look in `reports/<doc_id>/` for generated PDFs and diff images.

## Metrics information
- **BLEU** - a very popular metrics to compare translations. It primarily
  measures n-gram overlap between the candidate and references, giving a higher
  score for longer matching sequences. The score is on the scale from 0 to 100
  where more is better.

- **BERT** - BERTScore evaluates text similarity by calculating the cosine
  similarity between contextualized word embeddings (generated by BERT) of the
  candidate translation and the reference translation(s). Unlike n-gram overlap
  metrics like BLEU, BERTScore captures semantic similarity by considering the
  meaning of words in their context, making it a better indicator of fluency
  and adequacy. It computes separate precision, recall, and F1 scores based on
  this similarity. The score is on the scale from 0 to 1 where more is better.

- **TER** - (Translation Edit Rate) is a post-editing metric that measures
  the number of edits required to transform a machine translation into a
  perfect reference translation, normalized by the average number of words in
  the reference.
  Essentially, it calculates the minimum number of word-level operations (insertions, deletions, substitutions, and shifts of word sequences) needed to correct the machine output. A lower TER score indicates a higher quality translation, as fewer edits are needed.

# Literature
- [1: BERT score](https://huggingface.co/spaces/evaluate-metric/bertscore)
- [2: BLEU score](https://huggingface.co/spaces/evaluate-metric/bleu)
- [3: TER score](https://huggingface.co/spaces/evaluate-metric/ter)
